{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ferramentas para NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('cin').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, lower, monotonically_increasing_id, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('nlp').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.option(\"encoding\", \"ISO-8859-1\").text(['data.txt', 'class.txt', 'newtest.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"value\", regexp_replace(lower(df[\"value\"]), \"[$&+,:;=?@#|'<>.-^*()%!]\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.select('*').withColumn(\"id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|               value|         id|\n",
      "+--------------------+-----------+\n",
      "|classificacao reg...|          0|\n",
      "|aqui um arquivo d...| 8589934592|\n",
      "|    novão pra checar|17179869184|\n",
      "+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "Separação em tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>id</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>classificacao regressao modelos sao legais</td>\n",
       "      <td>0</td>\n",
       "      <td>[classificacao, regressao, modelos, sao, legais]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aqui um arquivo de texto</td>\n",
       "      <td>8589934592</td>\n",
       "      <td>[aqui, um, arquivo, de, texto]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>novão pra checar</td>\n",
       "      <td>17179869184</td>\n",
       "      <td>[novão, pra, checar]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        value           id  \\\n",
       "0  classificacao regressao modelos sao legais            0   \n",
       "1                    aqui um arquivo de texto   8589934592   \n",
       "2                            novão pra checar  17179869184   \n",
       "\n",
       "                                              words  \n",
       "0  [classificacao, regressao, modelos, sao, legais]  \n",
       "1                    [aqui, um, arquivo, de, texto]  \n",
       "2                              [novão, pra, checar]  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"value\", outputCol=\"words\")\n",
    "\n",
    "#extrai o token de acordo com o que foi especificado - aqui foi especificado\n",
    "#regexTokenizer = RegexTokenizer(inputCol=\"value\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "# pattern=\"\\\\w+\", gaps(False)\n",
    "\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "\n",
    "tokenized = tokenizer.transform(df)\n",
    "tokenized.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>words</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>classificacao regressao modelos sao legais</td>\n",
       "      <td>[classificacao, regressao, modelos, sao, legais]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aqui um arquivo de texto</td>\n",
       "      <td>[aqui, um, arquivo, de, texto]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>novão pra checar</td>\n",
       "      <td>[novão, pra, checar]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        value  \\\n",
       "0  classificacao regressao modelos sao legais   \n",
       "1                    aqui um arquivo de texto   \n",
       "2                            novão pra checar   \n",
       "\n",
       "                                              words  tokens  \n",
       "0  [classificacao, regressao, modelos, sao, legais]       5  \n",
       "1                    [aqui, um, arquivo, de, texto]       5  \n",
       "2                              [novão, pra, checar]       3  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized.select(\"value\", \"words\")\\\n",
    "    .withColumn(\"tokens\", countTokens(col(\"words\"))).toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Removendo Stop Words\n",
    "\n",
    "Stop Words são palavras que devem ser excluídas da entrada, normalmente porque as palavras aparecem com frequência e não têm tanto significado.\n",
    "\n",
    "StopWordsRemover toma como entrada uma sequência de strings (por exemplo, a saída de um Tokenizer) e elimina todas as palavras de parada das seqüências de entrada. A lista de Stop Words é especificada pelo parâmetro stopWords. As Stop Words padrões para alguns idiomas são acessíveis chamando StopWordsRemover.loadDefaultStopWords(idioma), para o qual as opções disponíveis são “danish”, “dutch”, “english”, “finnish”, “french”, “german”, “hungarian”, “italian”, “norwegian”, “portuguese”, “russian”, “spanish”, “swedish” and “turkish”. Um parâmetro booleano caseSensitive indica se as correspondências devem diferenciar maiúsculas de minúsculas (falso por padrão)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "language=\"portuguese\"\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\", stopWords=StopWordsRemover.loadDefaultStopWords(language))\n",
    "remover = remover.transform(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>id</th>\n",
       "      <th>words</th>\n",
       "      <th>filtered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>classificacao regressao modelos sao legais</td>\n",
       "      <td>0</td>\n",
       "      <td>[classificacao, regressao, modelos, sao, legais]</td>\n",
       "      <td>[classificacao, regressao, modelos, sao, legais]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aqui um arquivo de texto</td>\n",
       "      <td>8589934592</td>\n",
       "      <td>[aqui, um, arquivo, de, texto]</td>\n",
       "      <td>[aqui, arquivo, texto]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>novão pra checar</td>\n",
       "      <td>17179869184</td>\n",
       "      <td>[novão, pra, checar]</td>\n",
       "      <td>[novão, pra, checar]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        value           id  \\\n",
       "0  classificacao regressao modelos sao legais            0   \n",
       "1                    aqui um arquivo de texto   8589934592   \n",
       "2                            novão pra checar  17179869184   \n",
       "\n",
       "                                              words  \\\n",
       "0  [classificacao, regressao, modelos, sao, legais]   \n",
       "1                    [aqui, um, arquivo, de, texto]   \n",
       "2                              [novão, pra, checar]   \n",
       "\n",
       "                                           filtered  \n",
       "0  [classificacao, regressao, modelos, sao, legais]  \n",
       "1                            [aqui, arquivo, texto]  \n",
       "2                              [novão, pra, checar]  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remover.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gramas\n",
    "\n",
    "NGram toma como entrada uma seqüência strings (por exemplo, a saída de um Tokenizer). O parâmetro n é usado para determinar o número de termos em cada n-grama. A saída consistirá de uma sequência de n-gramas, onde cada n-grama é representado por uma string delimitada por espaço de n palavras consecutivas. Se a sequência de entrada contiver menos de n cadeias, nenhuma saída será produzida.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------+\n",
      "|ngrams                                                               |\n",
      "+---------------------------------------------------------------------+\n",
      "|[classificacao regressao, regressao modelos, modelos sao, sao legais]|\n",
      "|[aqui arquivo, arquivo texto]                                        |\n",
      "|[novão pra, pra checar]                                              |\n",
      "+---------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "ngram = NGram(n=2, inputCol=\"filtered\", outputCol=\"ngrams\")\n",
    "ngramDataFrame = ngram.transform(remover)\n",
    "\n",
    "ngramDataFrame.select(\"ngrams\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______\n",
    "# TF-IDF\n",
    "_______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            sentence|\n",
      "+-----+--------------------+\n",
      "|  0.0|Ola ja ouvi falar...|\n",
      "|  1.0|Eu gostaria de us...|\n",
      "|  2.0|Classificacao,reg...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0.0, \"Ola ja ouvi falar de Spark\"),\n",
    "    (1.0, \"Eu gostaria de usar datascience em tudo\"),\n",
    "    (2.0, \"Classificacao,regressao,modelos,sao,legais\")\n",
    "], [\"label\", \"sentence\"])\n",
    "\n",
    "sentenceData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|            sentence|               words|\n",
      "+-----+--------------------+--------------------+\n",
      "|  0.0|Ola ja ouvi falar...|[ola, ja, ouvi, f...|\n",
      "|  1.0|Eu gostaria de us...|[eu, gostaria, de...|\n",
      "|  2.0|Classificacao,reg...|[classificacao,re...|\n",
      "+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "wordsData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer\n",
    "\n",
    "O CountVectorizer selecionará as melhores palavras com um tamanho igual ao valor especificado pelo parâmetro vocabSize, ordenadas pela frequência do termo em todo o corpus. Um parâmetro opcional minDF também afeta o processo de ajuste, especificando o número mínimo dos documentos em que um termo deve aparecer para ser incluído no vocabulário. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\")\n",
    "#minDf o termo só aparece se estiver em mais documentos que o minimo.\n",
    "model = cv.fit(remover)\n",
    "\n",
    "result = model.transform(remover)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = ['data.txt', 'class.txt', 'newtest.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selected = result.select('features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "links = [list(map(int, selected.collect()[ind][0].toArray())) for ind, x in enumerate(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_vector = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for ind, x in enumerate(text):\n",
    "    dict_vector[text[ind]] = list(map(int, selected.collect()[ind][0].toArray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class.txt': [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
       " 'data.txt': [0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0],\n",
       " 'newtest.txt': [0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(int, result.select('features').collect()[1][0].toArray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arquivo',\n",
       " 'checar',\n",
       " 'legais',\n",
       " 'sao',\n",
       " 'pra',\n",
       " 'regressao',\n",
       " 'novão',\n",
       " 'classificacao',\n",
       " 'aqui',\n",
       " 'modelos',\n",
       " 'texto']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vocabulary"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
